# Testing Tasks Configuration
# Version: 1.0.1

analyze_code_task:
  description: >
    Analyze the provided code file for testing purposes.

    File path: {file_path}
    Language: {language}

    Code content:
    ```{language}
    {code_content}
    ```

    Your analysis MUST include:
    1. List all public functions/methods with their signatures
    2. Map input parameters and return types
    3. Count conditional branches (if/else, switch, ternary)
    4. Identify error handling blocks (try/catch/except)
    5. Note external dependencies and side effects
    6. Calculate complexity score (1-10)
    7. Prioritize which functions need tests most urgently

    Focus on testable units. Flag any code that's untestable
    (too many dependencies, side effects) with suggestions to refactor.
  expected_output: >
    JSON structure with file_path, language, list of functions with
    name, line_number, params, returns, branches, has_side_effects,
    dependencies, complexity, testability. Include total_complexity,
    priority_targets array, and warnings array.
  agent: code_analyzer_agent

write_tests_task:
  description: >
    Write {test_type} tests for the analyzed code.

    Test Framework: {test_framework}
    Language: {language}
    Coverage Target: minimum 80%

    Code to test:
    ```{language}
    {code_content}
    ```

    Use the code analysis from the previous task to understand
    which functions need tests and their complexity.

    REQUIREMENTS:
    1. Follow AAA pattern: Arrange, Act, Assert
    2. One assertion per test (single responsibility)
    3. Descriptive test names: test_[method]_[scenario]_[expected]
    4. Mock ALL external dependencies
    5. Include tests for:
       - Happy path (normal operation)
       - Edge cases (empty input, null, boundaries)
       - Error scenarios (exceptions, invalid input)
    6. Add docstrings explaining non-obvious test logic
    7. Group related tests in classes/describe blocks
    8. DO NOT use nested test classes - keep test structure FLAT

    DO NOT:
    - Leave TODO comments
    - Skip error handling tests
    - Use random values without seeding
    - Depend on test execution order
    - Create nested classes inside test classes
  expected_output: >
    Complete, runnable test file with:
    - All necessary imports
    - Fixtures/setup methods if needed
    - FLAT test classes organized by functionality (no nested classes)
    - At least 3 tests per public function
    - Comments for complex test scenarios

    The output should be valid {language} code that can be
    saved directly to a file and executed with pytest.
  agent: qa_test_agent

validate_tests_task:
  description: >
    Validate the generated tests for quality and correctness.

    Language: {language}
    Test Framework: {test_framework}

    Use the tests generated in the previous task.
    Compare them against the original code to verify coverage.

    Original code being tested:
    ```{language}
    {code_content}
    ```

    VALIDATION CHECKLIST:
    1. SYNTAX: Is the code syntactically valid?
    2. IMPORTS: Are all imports available and correct?
    3. COVERAGE: Are all public functions tested?
    4. EDGE CASES: Are boundary conditions covered?
    5. ISOLATION: Do tests depend on each other?
    6. DETERMINISM: Any random/time-dependent code?
    7. NAMING: Do names describe what's being tested?
    8. ASSERTIONS: Is each test asserting something meaningful?
    9. MOCKING: Are external dependencies properly mocked?
    10. READABILITY: Can a developer understand the tests?
    11. STRUCTURE: Are test classes flat (not nested)?
  expected_output: >
    Validation report with syntax_valid, imports_valid,
    estimated_coverage percentage, functions_tested array,
    functions_missing array, issues array with severity/test/issue/fix,
    quality_score, ready_for_execution boolean, and recommendations array.
  agent: test_validator_agent

# fix_tests_task is reserved for future use
# It will be triggered when validation finds critical issues
